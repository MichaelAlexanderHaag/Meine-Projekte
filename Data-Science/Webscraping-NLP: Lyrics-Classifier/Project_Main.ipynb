{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd6235-77b6-4b36-8e37-bc0e24cec8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq \n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6596a2-7562-4b69-97e1-c51a04b6dfeb",
   "metadata": {},
   "source": [
    "## Webscraping (OOP Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b5071-07f5-491e-9fb6-a8747d53d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricScraper: \n",
    "    def __init__(self, songs_pages:dict,save_destination:str,verbose=True):\n",
    "        self.save_destination = save_destination \n",
    "        self.verbose = verbose \n",
    "        self.songs_pages = songs_pages #Dict containing Artist and corresponding song pages\n",
    "        self.base_url = \"http://www.lyrics.com/\"\n",
    "        self.links = {} \n",
    "    \n",
    "    def extract_links_to_lyrics(self):\n",
    "        \"\"\"This function will go through the song page of each artist, extract the links to the lyrics and then set the links dictionary\"\"\" \n",
    "        for artist, songs_page in self.songs_pages.items(): \n",
    "            if self.verbose: \n",
    "                print(f\"Extracting Links from {artist} @ {songs_page}\")\n",
    "            songs_response = rq.get(songs_page)\n",
    "            songs_bs = BeautifulSoup(songs_response.text) \n",
    "            #Extract the Links \n",
    "            rows = songs_bs.find(\"table\",{\"class\":\"tdata\"}).findAll(\"tr\")\n",
    "            table_data = [t.td for t in rows ][1:] #Skip the header of the tabel\n",
    "            links = [td.a.get(\"href\")  for td in table_data]\n",
    "            #Clean the links \n",
    "            cleaned_links = LyricScraper.links_cleaner(links)\n",
    "            self.links[artist] = cleaned_links\n",
    "    \n",
    "    def links_cleaner(links:list): \n",
    "        \"\"\"This function does some filterning, many different links link to the same song lyrics, so get rid of them, remove remixes, acoustic versions and instrumentals as well\"\"\"\n",
    "        cleaned_links = []\n",
    "        titles=[]\n",
    "        pattern = r\"([Rr]emix|[Aa]coustic|[Ii]nstrumental|[Ff]eat.|[Mm]ix)\"\n",
    "        \n",
    "        #Get rid of all the links which link to the same song and get rid of remixes, accousitc versions and instrumentals\n",
    "        for element in links: \n",
    "            title_str = element.split(\"/\")[-1].lower()\n",
    "            if title_str in titles or re.search(pattern,title_str)!=None: \n",
    "                continue\n",
    "            else: \n",
    "                titles.append(title_str)\n",
    "                cleaned_links.append(element)\n",
    "        return cleaned_links\n",
    "    \n",
    "# A problem with this function is the repetion of code, since I don't have enough time I just went with it, but I know it's ugly design \n",
    "    def get_lyrics(self,ammount=\"all\"):\n",
    "        \"\"\"This function visits each site linked to  in self.links, extracts the lyrics and saves them separated by artist folders as a text file\n",
    "        You can also specifiy the ammount of songlyrics you want to download (this saves time and is usefull for balancing the models)\n",
    "        \"\"\" \n",
    "        number_of_links = [len(links) for links in self.links.values()]\n",
    "        \n",
    "        for artist, lyric_links in self.links.items(): \n",
    "            if ammount==\"all\": \n",
    "                for link in lyric_links: \n",
    "                    \n",
    "                    url = self.base_url+link \n",
    "                    title = url.split(\"/\")[-1].replace(\"+\",\" \")\n",
    "\n",
    "                    save_dir = os.path.join(self.save_destination,artist)\n",
    "                    if not os.path.exists(save_dir): \n",
    "                        os.mkdir(save_dir)\n",
    "                        \n",
    "                    if self.verbose:\n",
    "                        print(f\"Url updated to:{url}. Title:{title}. Artist:{artist}\")\n",
    "                        \n",
    "                    try: \n",
    "                        raw_html = rq.get(url).text\n",
    "                    except HTTPError as e:\n",
    "                        print(f\"An error occured: {e}\") \n",
    "                        return False \n",
    "                    bs_text = BeautifulSoup(raw_html)\n",
    "                    text = bs_text.pre.text \n",
    "                    text_file_name = save_dir+\"/\"+title\n",
    "                    with open(text_file_name,\"w\") as file: \n",
    "                        file.write(re.sub(r\"[^a-zA-Z0-9]+\", ' ', text))\n",
    "                        \n",
    "            elif ammount==\"balanced\": \n",
    "                if self.verbose: \n",
    "                    print(f\"Going for {min(number_of_links)} downloads. Artist:{artist}\")\n",
    "                    \n",
    "                for i in range(0,min(number_of_links)):\n",
    "                    url = self.base_url+lyric_links[i] \n",
    "                    title = url.split(\"/\")[-1].replace(\"+\",\" \")\n",
    "                               \n",
    "                    save_dir = os.path.join(self.save_destination,artist)\n",
    "                    if not os.path.exists(save_dir): \n",
    "                        os.mkdir(save_dir)\n",
    "                        \n",
    "                    if self.verbose:\n",
    "                        print(f\"Url updated to:{url}. Title:{title}. Artist:{artist}\")\n",
    "                    try: \n",
    "                        raw_html = rq.get(url).text\n",
    "                    except HTTPError as e:\n",
    "                        print(f\"An error occured: {e}\") \n",
    "                        return False \n",
    "                    bs_text = BeautifulSoup(raw_html)\n",
    "                    text = bs_text.pre.text \n",
    "                    text_file_name = save_dir+\"/\"+title\n",
    "                    with open(text_file_name,\"w\") as file: \n",
    "                        file.write(re.sub(r\"[^a-zA-Z0-9]+\", ' ', text))\n",
    "                    \n",
    "            elif isinstance(ammount,int) and (ammount <= min(number_of_links)):\n",
    "                if self.verbose: \n",
    "                    print(f\"Going for {ammount} downloads. Artist: {artist}\")\n",
    "                for i in range(0,ammount):\n",
    "                    url = self.base_url+lyric_links[i] \n",
    "                    title = url.split(\"/\")[-1].replace(\"+\",\" \")\n",
    "\n",
    "                    save_dir = os.path.join(self.save_destination,artist)\n",
    "                    if not os.path.exists(save_dir): \n",
    "                        os.mkdir(save_dir)\n",
    "                    if self.verbose:\n",
    "                        print(f\"Url updated to:{url}. Title:{title}. Artist:{artist}\")\n",
    "                    try: \n",
    "                        raw_html = rq.get(url).text\n",
    "                    except HTTPError as e:\n",
    "                        print(f\"An error occured: {e}\") \n",
    "                        return False \n",
    "                    bs_text = BeautifulSoup(raw_html)\n",
    "                    text = bs_text.pre.text \n",
    "                    text_file_name = save_dir+\"/\"+title\n",
    "                    with open(text_file_name,\"w\") as file: \n",
    "                        file.write(re.sub(r\"[^a-zA-Z0-9]+\", ' ', text))\n",
    "            else: \n",
    "                return False \n",
    "        print(\"Finished!\") \n",
    "        return True \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2788bd1-850a-4f65-833c-bdfbc79aeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir= \"/home/michaelhaag/Documents/Coding/Spiced_Code/Code-Repository/Week_04/Data\"\n",
    "ls = LyricScraper({\"Charli XCX\":\"https://www.lyrics.com/artist.php?name=Charli-XCX&aid=2391950&o=1\",\"Terror Jr\":\"https://www.lyrics.com/artist.php?name=Terror-Jr&aid=3252479&o=1\"},\n",
    "                  save_dir) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94edf00-b05d-4f18-8551-d76ed7f4a481",
   "metadata": {},
   "source": [
    "**Note**: I wrote a function that updated the url to visit the song pages of different artists in the following way:  \n",
    "1. User supplies a list of Names (e.g. [\"Charlie XCX\", \"Terror Jr\"]\n",
    "2. Update URL to artist page \n",
    "3. Go to Song Page   \n",
    "Unfortunately this didn't work because the pattern for generating the url for the artist pages wasn't general (e.g. lyrics.com/artists/Charlie-XCX -> Charlie XCX Artist Page, but lyrics.com/artists/Terror-Jr -/-> Terror Jr. Artist Page)   \n",
    "Therefore, I removed said function (maybe I'll implement it later on) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6f79c-2e01-4ea5-b4c2-911b053e8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.extract_links_to_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1909f72-98f3-42cb-80d6-ff6c4d2e99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.get_lyrics(ammount=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c4ff8-e459-44fa-8b38-ed97d6a72ad4",
   "metadata": {},
   "source": [
    "## Building a Natural Language Classifier Modell (basically just a Wrapper for the sklearn modells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ad11b-d071-433f-a5b8-f7c249c2920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsClassifier: \n",
    "    def __init__ (self,save_dir:str): \n",
    "        self.save_dir = save_dir #Where are the text files stored? \n",
    "        self.text_data = {} #This dictionary containts as keys artists and as values a list of their lyrics \n",
    "    \n",
    "    def read_text_files(self):\n",
    "        \"\"\"Sets the text_data dict by reading in the text files for each artist into a list and saving them as values in said dictionary\"\"\" \n",
    "        artist_folders = os.listdir(self.save_dir) \n",
    "        lyrics_dirs = [os.path.join(self.save_dir,artist) for artist in artist_folders]\n",
    "        for d in lyrics_dirs: \n",
    "            artist_name = d.split(\"/\")[-1]\n",
    "            lyrics = []\n",
    "            for text_file in os.listdir(d):\n",
    "                #print(f\"Reading in: {text_file} in directory {d}\")\n",
    "                with open(d+\"/\"+text_file,\"r\") as file: \n",
    "                    text=file.read()\n",
    "                    lyrics.append(text)\n",
    "            self.text_data[artist_name]=lyrics\n",
    "    \n",
    "    def get_corpus_and_labels(self): \n",
    "        corpus=[]\n",
    "        lables =[]\n",
    "        for artist,lyrics in self.text_data.items():\n",
    "            for lyric in lyrics:\n",
    "                corpus.append(lyric)\n",
    "                lables.append(artist)\n",
    "        return (corpus,lables)\n",
    "    \n",
    "    def train_logreg_model(self,corpus,label): \n",
    "        tf_vec = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2),max_df=0.9)\n",
    "        lrm = LogisticRegression()\n",
    "        model = make_pipeline(tf_vec,lrm)\n",
    "        model.fit(corpus,labels)\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    def train_nb_model(self,corpus,label): \n",
    "        tf_vec = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2),max_df=0.9)\n",
    "        nbm = MultinomialNB(alpha=1)\n",
    "        model = make_pipeline(tf_vec,nbm)\n",
    "        model.fit(corpus,labels)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def predict(self,model,new_text): \n",
    "        new_text = [new_text]\n",
    "        prediction = model.predict(new_text) \n",
    "        return prediction[0]\n",
    "            \n",
    "                \n",
    "                    \n",
    "\n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4df394-a14d-468a-92b7-6ffc05698983",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm = LyricsClassifier(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a5f3b-65f7-4b09-88dd-fa4341b6810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm.read_text_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6e700-944f-4543-925d-9500bd45d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,labels = lcm.get_corpus_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d9259-6ec9-423b-9fca-09ad28174c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = lcm.train_logreg_model(corpus,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7f1ee-7a56-417e-835e-83482dcaa3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm.predict(log_reg_model,\"I want to meet my maker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c6436-d1bc-4037-a481-8237250025ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm2 = LyricsClassifier(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5039b-b193-4a50-b706-8dc54efe68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm.read_text_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f88d71-a155-4009-93b6-23c9a94d2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,labels = lcm.get_corpus_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8845df-0d7f-48ad-b996-e547db380dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = lcm.train_nb_model(corpus,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63a255-dafb-4895-b154-85f3ee58f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm.predict(nb_model,\"Smile mouth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f1ea0-ac57-4c86-968d-c91024d5ef6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Count Vectorizer (just playing around) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd997c-9fc5-498e-aec9-10063304cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=\"english\",ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca33233-1605-4aee-a00e-5cf99ab1b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_charli = lcm.text_data[\"Charli XCX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600347da-5a53-4747-9ebb-6870a03b5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(lyrics_charli) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a391bf-fd1c-4c18-a121-38c7ac4fa99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transform(lyrics_charli).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4356a82-a0c4-4f21-9e03-0539401941d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lyrics = cv.transform(lyrics_charli).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d9b7e-9337-472d-8c5c-bc525040d141",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The TF-Idf Transformer (just playing around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee4bf8-f949-4694-b491-2fca148c4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7af9a4-8839-4a98-ac4b-5aae4498bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec.fit(lyrics_charli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03cfa6-ba93-483a-a0cd-4965dff356fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = tf_vec.transform(lyrics_charli).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff7911-700e-4ac5-aa14-f78387639032",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lyrics Classification (manually) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78480-9ff8-42dc-8700-d0ca4dd98b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcm.text_data[\"Charli XCX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7e719-d2b0-477c-ba39-7279c6ea819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcm.text_data[\"Terror Jr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69422242-55f1-4730-9ff4-39229726dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_corpus = lcm.text_data[\"Charli XCX\"] + lcm.text_data[\"Terror Jr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c23b8b-2ec3-482b-bab3-dd808461f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lyrics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90c17d-2d9d-4cda-9406-44fe0c3cc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Charlie XCX\"] * 34 + [\"Terror Jr\"] * 34 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6559ba-3f54-4f22-9b10-b6c0092ea0d2",
   "metadata": {},
   "source": [
    "### TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cca84-a932-45d1-8312-7ec65fdc5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec =TfidfVectorizer(stop_words=\"english\",ngram_range=(1,2),max_df=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5df3e-bdfe-4250-b6fd-d585fb380de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec.fit(lyrics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec08a83-b5fb-49ae-b1d7-fb6df483847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_corpus_trans = tf_vec.transform(lyrics_corpus).todense()\n",
    "len(lyrics_corpus_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6b10-8485-45a6-b2fd-ab2c06477da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Logistic Regression Modell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894135f-3a9b-4c07-bfd8-e87aa7517e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d0deb-c4fb-4c33-956a-5507dfe24a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(np.asarray(lyrics_corpus_trans),labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598b0d0-3abf-4fa9-8da9-f14a070586e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297cfe5-0ce1-4fe5-8bc4-967c26b7a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"smash into pieces \"\n",
    "lr.predict(tf_vec.transform([sentence]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
